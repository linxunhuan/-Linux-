# 微内核
## 为什么有微内核
+ 微内核是人们在思考操作系统内核应该做什么事情的过程中设计出来的
+ 首先，说明一下操作系统的传统实现方式以及应该具备的功能
    + Linux，Unix，XV6称为用传统方式实现的操作系统
    + 另一个形容这些操作系统的词是monolithic
      + monolithic的意思是指操作系统内核是一个完成了各种事情的大的程序
      + 实际上，这也反应了人们觉得内核应该具备什么样的功能
        + 类似于Linux的典型操作系统内核提供了功能强大的抽象
        + 它们选择提供例如文件系统这样一个极其复杂的组件
          + 并且将文件，目录，文件描述符作为文件系统的接口
          + 而不是直接将磁盘硬件作为接口暴露给应用程序
        + monolithic kernel通常拥有例如文件系统这样强大的抽象概念
---------------------------------------
### 好处一：这些高度抽象的接口通常是可移植的
+ 可以在各种各样的存储上实现文件和目录
+ 可以使用文件和目录而不用担心它们是运行在什么牌子的磁盘
    + 但是因为文件系统接口是高度抽象的，所以它们都拥有相同的接口
    + 所以这里的一个好处是可以获取可移植性
    + 可以在不修改应用程序的前提下，将其运行在各种各样的硬件之上
### 好处二：Linux/Unix提供地址空间的抽象而不是直接访问MMU硬件的权限
+ 这不仅可以提供可移植性，并且也可以向应用程序隐藏复杂性
  + 所以操作系统具备强大抽象的另一个好处是，它们可以向应用程序隐藏复杂性
    + 举个例子，XV6提供的文件描述符非常简单，你只需要对文件描述符调用read/write就可以
### 好处三：这里的强大的抽象还可以帮助管理共享资源
+ 例如我们将内存管理委托给了内核，内核会跟踪哪些内存是空闲的
+ 类似的，内核还会跟踪磁盘的哪个部分是空闲的，磁盘的哪个部分正在被使用
+ 这样应用程序就不用考虑这些问题，所以这可以帮助简化应用程序
+ 同时也可以提供健壮性和安全
  + 因为如果允许应用程序决定磁盘的某个位置是否是空闲的
  + 那么应用程序或许可以使用一个已经被其他应用程序使用的磁盘位置
  + 所以，内核管理硬件资源可以提供资源共享能力和安全性
### 好处四：所有这些功能都在一个程序里面
+ 所有的内核子系统，例如文件系统，内存分配，调度器，虚拟内存系统都是集成在一个巨大的程序中的一个部分
  + 这意味着它们可以访问彼此的数据结构
  + 进而使得依赖多个子系统的工具更容易实现
  + 举个例子，exec系统调用依赖文件系统
    + 因为它要从磁盘中读取二进制文件并加载到内存中
    + 同时它也依赖内存分配和虚拟内存系统
      + 因为它需要设置好新的进程的地址空间，但是它的实现是相对简单的
      + 在XV6或者Linux中做到这些完全没问题，因为这些操作系统已经在内核程序中包含了文件系统和虚拟内存系统
      + 但是如果严格分隔了文件系统和虚拟内存系统，那么实现类似exec的系统调用将会难得多
      + 在一个monolithic操作系统中，因为本身就是一个大的程序，实现起来会容易的多
### 好处四：内核的所有代码都以完整的硬件权限在运行
+ 整个XV6都运行在Supervisor mode
  + 这意味着你可以读写任意内存地址
  + 并且所有的内核代码都以最大的权限在运行
  + Linux操作系统也是这样
----------------------------------
+ 然而，对于传统的monolithic kernel，也有一些缺点
  + 这也是之所以会出现其他内核架构，比如说微内核的原因
  + 所以这里的问题是，为什么不在所有的场合使用monolithic kernel呢？
### 缺点一：它们大且复杂
+ Linux总是有数十万到数百万行代码
  + Linux的一部分可以查看Linux的另一个部分的数据，的确使得编程更加容易
  + 但是同样也使得内部代码有大量的交互和依赖
  + 有的时候查看并弄明白Linux代码会有点挑战
    + 任何时候你有了一个大的程序，尤其它们还具有复杂的结构，你都会有Bug，操作系统内核也不例外
    + 如果你使用了大的内核，你不可避免的会遇到Bug和安全漏洞
### 缺点二：随着时间的推移，它们倾向于发展成拥有所有的功能
+ Linux应用在各种场合中，从移动电话到桌面工作站，从笔记本电脑到平板电脑，从服务器到路由器
  + 但是，例如web server程序不太会需要用到Linux中非常复杂的声卡支持
  + 对于各种不同的场景都能支持，或许就不能对某些特定场景进行优化
    + 当尝试快速运行一些程序时，如果程序只做一两件事情是极好的，因为这样你就可以专注在优化一两个代码路径上
    + 但是如果你的程序想要做上千件事情，优化会更加难,这里有大量的内容或许并不是必须的
### 缺点三：会削弱一些复杂的抽象能力
+ 在内核中会有大量的设计考虑，应用程序需要遵守这些设计并与之共存
+ 反之，在一个理想世界中，应用程序或许可以做更多的决定
  + 举个例子，在Unix中，可以wait子进程，比如说fork出来的子进程，但是不能wait其他进程
  + 或许你会想要wait孙子进程或者一个不相关的进程，但是这是不可能的
  + 或许你会想要更改其他进程的地址空间，比如说替其它受你控制的进程调用mmap，但是这也不可能
    + mmap只能修改你自己的地址空间，但是不能修改其他进程的地址空间
  + 或许你是个数据库，你在磁盘上有B树索引
    + 你或许知道很多快速展开B树的方法
      + 但是当你读写文件系统中的文件时
      + 文件系统并不知道你正在读写一个B树
      + 以及如何更快的在磁盘上展开B树
    + 所以如果你是个数据库的话，你或许很高兴文件系统可以任你摆布，但是文件系统并不会按照你想要的方式工作
### 缺点三：可扩展性（Extensibility）
+ 应用程序或许想要实时更改内核
  + 比如说向内核下载代码并更改内核的工作方式
  + 这样数据库或许就可以更改数据在磁盘上的分布方式
+ 至少在10年前，monolithic kernel没有任何功能可以支持这里的Extensibility
  + 只能使用内核提供的能力
## Micro kernel
+ 微内核的核心就是实现了:
  + IPC（Inter-Process Communication）
  + 线程
  + 任务的tiny kernel
+ 微内核只提供了进程抽象和通过IPC进程间通信的方式，除此之外别无他物
  + 任何你想要做的事情，例如文件系统
  + 你都会通过一个用户空间进程来实现，完全不会在内核中实现
+ 整个计算机还是分为两层
  + 下面是kernel
  + 上面是用户空间
    + 在用户空间或许还是会有各种各样常见的程序，例如VI，CC，桌面系统
    + 除此之外，在用户空间还会有文件系统以及知道如何与磁盘交互的磁盘驱动
    + 或许我们还会有一个知道如何进行TCP通信的网络协议栈
    + 或许还有一个可以实现虚拟内存技巧的虚拟内存系统
+ 当文本编辑器VI需要读取一个文件时，它需要与文件系统进行交互
  + 所以它通过IPC会发送一条消息到文件系统进程
  + 文件系统进程中包含了所有的文件系统代码，它知道文件，目录的信息
  + 文件系统进程需要与磁盘交互，所以它会发送另一个IPC到磁盘驱动程序
  + 磁盘驱动程序再与磁盘硬件进行交互，之后磁盘驱动会返回一个磁盘块给文件系统
  + 之后文件系统再将VI请求的数据通过IPC返回给VI
<img src=".\picture\image176.png">

+ 在内核中唯一需要做的是支持进程/任务/线程，以及支持IPC来作为消息的传递途径
+ 除此之外，内核不用做任何事情
+ 内核中没有任何文件系统，没有任何设备驱动，没有网络协议栈
  + 所有这些东西以普通用户进程在运行
## 构建微内核的动机
+ 人们期望通过使用微内核可以得到的特性，其实都是来自于代码少的优势：
  + bug少
  + 更容易被优化
  + 会运行的更快
  + 自带了少得多的设计限制，进而使得应用程序的设计限制也更少
    + 这样给应用程序提供了更多的灵活性
-------------------------------
+ 微内核的优势：
  + 有很多我们习惯了位于内核的功能和函数，现在都运行在用户空间
    + 这种将内核拆分，并在用户空间的不同部分运行，可以使得代码更模块化
      + 比如说在用户空间运行文件系统服务
  + 用户空间代码通常会比内核更容易被修改，调整和替换，所以它更容易被定制化
  + 将操作系统放在用户空间，或许可以使得它更加的健壮
    + 如果内核出错了，通常需要panic并重启
      + 因为如果内核有Bug，并且会随机更改数据，那就不能信任内核了
    + 然而，如果将内核运行成一些用户空间的服务，其中一个出现故障
      + 比如说除以0，索引了一个野指针
      + 或许只有这一个服务会崩溃，操作系统的剩余部分还是完好的，这样你可以只重启那一个服务
      + 所以，将操作系统的功能移到用户进程可以使得系统更加健壮
    + 这对于驱动来说尤其明显
      + 内核中大部分Bug都在硬件驱动中
      + 如果我们能将设备驱动从内核中移出的话
      + 那么内核中可能会有少的多的Bug和Crash
  + 最后一个优势是，可以在微内核上模拟或者运行多个操作系统
    + 所以尽管微内核几乎不做任何事情，你还是可以在它之上运行一个Unix系统之类的
    + 或许还可以在同一个机器上运行超过一个操作系统
----------------------------------
+ 微内核的挑战
  + 什么才是有用的系统调用的最小集？
    + 需要这些系统调用API尽可能的少
    + 但是又需要基于这些API构建一些非常复杂的功能
      + 因为即使内核没有做太多工作，最终还是要运行程序
    + 或许想要在微内核之上运行Unix，需要能执行类似fork，mmap的工作
      + 所以底层的系统调用在简单的同时，需要能够足够强大以支持人们需要做的各种事情
      + 比如说exec，fork，copy-on-write fork，mmap file
  + 但是内核又完全不知道文件和文件系统
    + 所以现在系统调用需要支持exec，而内核又不知道文件
    + 微内核或许会非常简单，但是我们仍然需要开发一些用户空间服务来实现操作系统的其他部分
  + 最后，微内核的设计需要进程间通过IPC有大量的通信
    + 所以有很大的需求使得IPC能够足够的快
    + 我们会好奇，IPC可以足够的快来使得微内核足够有竞争力吗？
  + 有关性能，不仅与IPC的速度相关
    + 通常来说，monolithic kernel可以获得更好的性能
    + 是因为它里面的文件系统代码和与虚拟内存代码可以直接交互，它们位于一个巨大的程序中
    + 但是如果需要将这些模块都拆分开成为不同的服务，那么在集成的时候就有更少的机会可以优化，这或许会影响性能
## L4 micro kernel
+ 它在内部有一个叫做Task或者地址空间的概念
    + 对应了Uinx内的进程概念
  + Task包含了一些内存，地址从0开始，并且可以像进程一样执行指令
    + 区别于XV6的是，每个Task可以有多个线程
    + L4会调度每个Task内的多个线程的执行
  + 这样设计的原因是
    + 可以非常方便地用线程来作为组织程序结构的工具
    + L4内核知道Task，知道线程，也知道地址空间，这样你就可以告诉L4如何映射地址空间内的内存Page
+ 另一个L4知道的事情是IPC
  + 每一个线程都有一个标识符，其中一个线程可以说，我想要向拥有这个标识符的另一个线程发送几个字节
  + 这里的Task，线程，地址空间，IPC是L4唯一有的抽象
+ 这里涉及到的系统调用有：
  + Threadcreate系统调用，你提供一个地址空间ID并要求创建一个新的线程。如果地址空间或者Task不存在，系统调用会创建一个新的Task。所以这个系统调用即可以创建线程，又可以创建Task
  + Send/Recv IPC系统调用
  + Mapping系统调动可以映射内存Page到当前Task或者其他Task的地址空间中
    + 你可以要求L4来改变当前Task的地址空间和Page Table
    + 如果有足够的权限，你也可以要求L4改变其他Task的地址空间
      + 这实际上是通过IPC完成的，发送一个特殊的IPC消息到目标线程，内核可以识别这个IPC消息，并会修改目标线程的地址空间
      + 如果创建一个新的线程，新线程最开始没有任何内存
        + 先调用Threadcreate系统调用来创建新的线程，新的Task和地址空间
        + 然后创建一个特殊 IPC，将内存中的一部分
          + 其中包含了指令和数据，映射到新的Task的地址空间中
        + 之后再发送一个特殊的Start IPC消息到这个新的Task
          + 其中包含了你期望新的Task开始执行程序的程序计数器和Stack Pointer
        + 之后新的Task会在你设置好的内存中，从你要求的程序计数器位置开始执行
  + Privileged Task可以将硬件控制寄存器映射到自己的地址空间中
    + 所以L4并不知道例如磁盘或者网卡的设备信息
    + 但是实现了设备驱动的用户空间软件可以直接访问设备硬件
  + 可以设置L4将任何一个设备的中断转换成IPC消息
    + 这样，运行设备驱动的Task不仅可以读写了设备
    + 并且也可以设置L4将特定设备的中断通过IPC消息发送给自己
  + 最后，一个Task可以设置L4内核通知自己有关另一个Task的Page Fault
    + 所以如果一个Task发生了Page Fault
    + L4会将Page Fault转换成一个IPC消息
    + 并发送给另一个指定的Pager Task
    + 每一个Task都有个与之关联的Pager Task用来处理自己相关的Page Fault
      + 这就是关联到Page Fault的方法，通过它可以实现类似copy-on-write fork或者lazy allocation
----------------------------
+ 以上就是内核的内容，L4里面不包含其他的功能
  + 没有文件系统，没有fork/exec系统调用
  + 除了这里非常简单的IPC之外，没有其他例如pipe的通信机制，没有设备驱动，没有网络的支持等等
  + 任何其他想要的功能，需要以用户空间进程的方式提供
+ L4能提供的一件事情是完成线程间切换
  + L4会完成线程调度和context switch，来让多个线程共用一个CPU
    + 实现的方式：
      + L4会为每个Task保存寄存器
      + 当它执行一个线程时，它会跳到用户空间，切换到那个线程对应Task的Page Table
      + 之后那个线程会在用户空间执行一会
      + 之后或许会有一个定时器中断，会使代码执行返回到L4内核
      + L4会保存线程的用户寄存器，然后在一个类似于XV6的线程调度循环中，选择一个Task来运行
  + 通过将这个Task之前保存的寄存器恢复出来
    + 切换Page Table，就可以跳转到Task中再运行一会
    + 直到再发生另一个定时中断，或者当前Task出让了CPU
+ 如果一个进程触发了Page Fault
  + 通过trap走到了内核，内核会将Page Fault转换成IPC消息并发送到指定的Pager Task
  + 并告诉Pager Task是哪个线程的哪个地址触发了Page Fault
  + 在Pager Task中，如果它实现了lazy allocation
    + 那么它会负责从L4分配一些内存
    + 向触发Page Fault的Task发送一个特殊的IPC，来恢复程序的运行
    + 所以Pager Task实现了XV6或者Linux在Page Fault Handler中实现的所有功能
      + 如果你想的话，你可以在Pager Task中实现copy-on-write fork或者memory mapped files
      + Pager Task可以实现基于Page Fault的各种技巧
## 通过内核设计改进 IPC
+ 接下来我们讨论微内核里面一个非常重要的问题：
  + IPC的速度
+ 假设我们有两个进程，P1和P2，P1想要给P2发送消息
  + 使用send系统调用，传入你想将消息发送到的线程的ID，以及你想发送消息的指针
    + 这个系统调用会跳到内核中，假设我们是基于XV6的pipe来实现，那么这里会有一个缓存
    + 或许P2正在做一些其他的事情，并没有准备好处理P1的消息
      + 所以消息会被先送到内核的缓存中
      + 所以当调用send系统调用，它会将你的消息追加到一个缓存中等待P2来接收它
    + 在实际中，几乎很少情况你会只想要发送一个消息，你几乎总是想要能再得到一个回复
      + 所以P1在调用完send系统调用之后，会立即调用recv来获取回复
      + 但是现在让我们先假设我们发送的就是单向的IPC消息，send会将你的消息追加到位于内核的缓存中
        + 我们需要从用户空间将消息逐字节地拷贝到内核的缓存中
        + 之后再返回，这样P1可以做一些其他的事情，或许是做好准备去接受回复消息
    + 过了一会，P2可以接收消息了
      + 它会调用recv系统调用，这个系统调用会返回发送消息线程的ID
      + 并将消息从内核拷贝到P2的内存中
      + 所以这里会从内核缓存中取出最前的消息，并拷贝到P2的内存中，之后再返回
+ 这种方式被称为异步传输
  + 因为P1发完消息之后，只是向缓存队列中追加了一条消息，并没有做任何等待就返回了
  + 同时这样的系统这也被称作是buffered system
    + 因为在发送消息时，内核将每条消息都拷贝到了内部的缓存中
    + 之后当接收消息时，又从buffer中将消息拷贝到了目标线程
    + 所以这种方法是异步buffered
  + 如果P1要完成一次完整的消息发送和接收，那么可以假设有两个buffer
    + 一个用来发送消息
    + 一个用来接收消息
  + P1会先调用send，send返回之后
  + 之后P1会立即调用recv，recv会等待接收消息的buffer出现数据，所以P1会出让CPU
    + 在一个单CPU的系统中，只有当P1出让了CPU，P2才可以运行
+ 论文中的讨论是基于单CPU系统
  + 所以P1先执行，之后P1不再执行，出让CPU并等待回复消息
  + 这时，P2才会被调度，之后P2调用recv，拷贝消息
  + 之后P2自己再调用send将回复消息追加到buffer
    + 之后P2的send系统调用返回
  + 假设在某个时间，或许因为定时器中断触发导致P2出让CPU
    + 这时P1可以恢复运行，内核发现在接收消息buffer有了一条消息，会返回到用户空间的P1进程
+ 这意味着在这个慢的设计中，为了让消息能够发送和回复，将要包含：
  + 4个系统调用，两个send，两个recv
  + 对应8次用户空间内核空间之间的切换，而每一次切换明显都会很慢
  + 在recv的时候，需要通过sleep来等待数据出现
  + 并且需要至少一次线程调度和context switching来从P1切换到P2
+ 每一次用户空间和内核空间之间的切换和context switching都很费时
  + 因为每次切换，都需要切换Page Table，进而清空TLB，也就是虚拟内存的查找缓存
  + 所以这是一种非常慢的实现方式，它包含了大量的用户空间和内核空间之间的切换、消息的拷贝、缓存的分配等等
  + 实际中，对于这里的场景：
    + 发送一个消息并期待收到回复，你可以抛开这种方法并获得简单的多的设计，L4就是采用了后者
---------------------------------------------------
+ 有关简单的设计在一篇著名的论文中有提到,它有几点不同：

+ 这里不会丢下消息并等待另一个进程去获取消息，这里的send会等待消息被接收，并且recv会等待回复消息被发送
+ 如果我是进程P1，我想要发送消息
  + 我会调用send
  + send并不会拷贝我的消息到内核的缓存中，P1的send会等待P2调用recv
    + P2要么已经在内核中等待接收消息，要么P1的send就要等P2下一次调用recv
    + 当P1和P2都到达了内核中，也就是P1因为调用send进入内核，P2因为调用recv进入内核，这时才会发生一些事情
      + 这种方式快的一个原因是:
        + 如果P2已经在recv中，P1在内核中执行send可以直接跳回到P2的用户空间
        + 从P2的角度来看，就像是从recv中返回一样，这样就不需要context switching或者线程调度
      + 相比保存寄存器，出让CPU，通过线程调度找到一个新的进程来运行，这是一种快得多的方式
      + P1的send知道有一个正在等待的recv，它会立即跳转到P2，就像P2从自己的recv系统调用返回一样
  + 这种方式也被称为unbuffered。它不需要buffer一部分原因是因为它是同步的

+ 当send和recv都在内核中时
  + 内核可以直接将消息从用户空间P1拷贝到用户空间P2，而不用先拷贝到内核中，再从内核中拷出来
  + 因为现在消息收发的两端都在等待另一端系统调用，这意味着它们消息收发两端的指针都是确定的
  + recv会指定它想要消息被投递的位置
  + 所以在这个时间点，我们知道两端的数据内存地址，内核可以直接拷贝消息，而不是需要先拷贝到内核
+ 如果消息超级小
  + 比如说只有几十个字节，它可以在寄存器中传递，而不需要拷贝
    + 可以称之为Zero Copy
  + 前面说过，发送方只会在P2进入到recv时继续执行
    + 之后发送方P1会直接跳转到P2进程中
    + 从P1进入到内核的过程中保存P1的用户寄存器
      + 这意味着，如果P1要发送的消息很短，它可以将消息存放到特定的寄存器中
    + 当内核返回到P2进程的用户空间时，会恢复保存了的寄存器
      + 这意味着当内核从recv系统调用返回时，特定寄存器的内容就是消息的内容
      + 因此完全不需要从内存拷贝到内存，也不需要移动数据
      + 消息就存放在寄存器中,可以非常快的访问到
+ 对于非常长的消息
  + L4可以在一个IPC消息中携带一个Page映射，所以对于巨大的消息
  + 比如说从一个文件读取数据，可以发送一个物理内存Page
    + 这个Page会被再次映射到目标Task地址空间，这里也没有拷贝
    + 这里提供的是共享Page的权限
  + 所以短的消息很快，非常长的消息也非常快
  + 对于长的消息，需要调整目的Task的Page Table，但是这仍然比拷贝快的多
+ 最后一个L4使用的技巧是
  + 如果它发现这是个RPC，有request和response
  + 并且有非常标准的系统调用包括了send和recv
  + 或许会结合这两个系统调用，以减少用户态和内核态的切换
  + 所以对于RPC这种特别的场景，同时也是人们使用IPC的一个常见场景
    + 有一个call系统调用，它基本上结合了send和recv
    + 区别是这里不会像两个独立的系统调用一样，先返回到用户空间，再次进入到内核空间
    + 在消息的接收端，会有一个sendrecv系统调用将回复发出
      + 之后等待来自任何人的request消息
      + 这里基本是发送一个回复再加上等待接收下一个request
      + 这样可以减少一半的内核态和用户态切换
## 在 L4 微内核上运行 Linux
现在的微内核大概只有一个完整操作系统的百分之几，我们该怎么处理操作系统剩下的部分？
+ L4微内核位于底部，但是同时，一个完整的Linux作为一个巨大的服务运行在用户空间进程中
  + (我感觉，这不是就是模仿虚拟机，写实验的QEMU不就是干这个的)
+ 这里的实现方式是：
  + 将Linux内核作为一个L4 Task运行
  + 每一个Linux进程又作为一个独立的L4 Task运行
  + 所以当登录到Linux中时，你要它运行一个Shell或者terminal
    + 它会在用户空间创建一个L4 Task来运行这个Linux程序
  + 所以现在有一个Task运行Linux，以及N个Task来运行每一个你在Linux中启动的进程
+ Linux不会直接修改进程的Page Table
  + 而是会向L4发送正确的IPC让L4来修改进程的Page Table
---------------------------
+ 当VI想要执行一个系统调用时
  + VI并不知道它运行在L4之上，所有的程序都以为它们运行在Linux中
  + 当VI要执行系统调用时，L4并不支持
    + 因为VI要执行的是Linux系统调用而不是L4系统调用
  + 所以对于Linux进程，会有一个小的库与之关联
    + 这个库会将类似于fork，exec，pipe，read，write的系统调用
    + 转换成发送到Linux kernel Task的IPC消息
    + 并等待Linux kernel Task的返回
    + 然后再返回到进程中
  + 从VI的角度看起来好像就是从系统调用返回了
    + 所以这些小的库会将系统调用转成发送到Linux kernel Task的IPC消息
+ 这意味着，如果Linux kernel Task没有做其他事情的话
  + 它会在一个recv系统调用中等待接收从任何一个进程发来的下一个系统调用请求IPC
+ 这导致了这里的Linux和普通的Linux明显不同的工作方式
  + 在普通的Linux中，就像XV6一样，会有一个内核线程对应每一个用户空间进程
  + 当用户空间进程调用系统调用时，内核会为这个系统调用运行一个内核线程
  + 并且，在普通的Linux中，如果内核在内核线程之间切换
    + 这基本上意味着从一个用户进程切换到另一个用户进程
    + 所以这里Linux kernel的内核线程以及当Linux完成工作之后要运行的用户进程之间有一对一的关系
+ 在这里架构中，这种一对一的关系断了
  + 这里的Linux kernel运行在一个L4线程中
  + 然而，这个线程会使用与XV6中的context switching非常相似的技术
    + 在与每个用户进程对应的内核线程之间切换
  + 不过这些内核线程完全是在Linux中实现的，与L4线程毫无关系
    + 唯一的L4线程就是运行了Linux kernel的控制线程
+ 但是哪个用户进程可以运行，是由L4决定的
  + 所以在这里的设置中，Linux kernel或许在内核线程中执行来自VI的系统调用
  + 同时，L4又使得Shell在用户空间运行了
  + 这在XV6或者Linux极不可能发生
    + 在这两个系统中，活跃的内核线程和用户进程有直接的对应关系
  + 而L4会运行它喜欢的任何Task
    + 因为Linux kernel中的内核线程都是私有的实现
    + Linux可以同时执行不同阶段的多个系统调用，或许一个进程在它的内核线程中在等待磁盘
      + 这时Linux可以运行另一个进程的内核线程来处理另一个进程的系统调用